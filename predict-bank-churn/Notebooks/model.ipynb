{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_tabular\n",
    "import pytorch_tabular.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_tabular.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Accuracy: 0.2667\n",
      "Epoch 2/100, Accuracy: 0.4667\n",
      "Epoch 3/100, Accuracy: 0.8000\n",
      "Epoch 4/100, Accuracy: 0.9333\n",
      "Epoch 5/100, Accuracy: 0.9333\n",
      "Epoch 6/100, Accuracy: 0.9333\n",
      "Epoch 7/100, Accuracy: 0.9333\n",
      "Epoch 8/100, Accuracy: 0.9667\n",
      "Epoch 9/100, Accuracy: 0.9667\n",
      "Epoch 10/100, Accuracy: 0.9667\n",
      "Epoch 11/100, Accuracy: 0.9667\n",
      "Epoch 12/100, Accuracy: 0.9667\n",
      "Epoch 13/100, Accuracy: 0.9667\n",
      "Epoch 14/100, Accuracy: 0.9667\n",
      "Epoch 15/100, Accuracy: 1.0000\n",
      "Epoch 16/100, Accuracy: 1.0000\n",
      "Epoch 17/100, Accuracy: 1.0000\n",
      "Epoch 18/100, Accuracy: 0.9667\n",
      "Epoch 19/100, Accuracy: 0.9667\n",
      "Epoch 20/100, Accuracy: 0.9667\n",
      "Epoch 21/100, Accuracy: 0.9667\n",
      "Epoch 22/100, Accuracy: 0.9667\n",
      "Epoch 23/100, Accuracy: 1.0000\n",
      "Epoch 24/100, Accuracy: 1.0000\n",
      "Epoch 25/100, Accuracy: 0.9667\n",
      "Epoch 26/100, Accuracy: 0.9667\n",
      "Epoch 27/100, Accuracy: 1.0000\n",
      "Epoch 28/100, Accuracy: 1.0000\n",
      "Epoch 29/100, Accuracy: 0.9667\n",
      "Epoch 30/100, Accuracy: 0.9667\n",
      "Epoch 31/100, Accuracy: 0.9667\n",
      "Epoch 32/100, Accuracy: 1.0000\n",
      "Epoch 33/100, Accuracy: 0.9667\n",
      "Epoch 34/100, Accuracy: 1.0000\n",
      "Epoch 35/100, Accuracy: 1.0000\n",
      "Epoch 36/100, Accuracy: 1.0000\n",
      "Epoch 37/100, Accuracy: 1.0000\n",
      "Epoch 38/100, Accuracy: 1.0000\n",
      "Epoch 39/100, Accuracy: 1.0000\n",
      "Epoch 40/100, Accuracy: 1.0000\n",
      "Epoch 41/100, Accuracy: 0.9667\n",
      "Epoch 42/100, Accuracy: 0.9667\n",
      "Epoch 43/100, Accuracy: 0.9667\n",
      "Epoch 44/100, Accuracy: 1.0000\n",
      "Epoch 45/100, Accuracy: 0.9667\n",
      "Epoch 46/100, Accuracy: 0.9667\n",
      "Epoch 47/100, Accuracy: 0.9667\n",
      "Epoch 48/100, Accuracy: 1.0000\n",
      "Epoch 49/100, Accuracy: 1.0000\n",
      "Epoch 50/100, Accuracy: 1.0000\n",
      "Epoch 51/100, Accuracy: 1.0000\n",
      "Epoch 52/100, Accuracy: 1.0000\n",
      "Epoch 53/100, Accuracy: 1.0000\n",
      "Epoch 54/100, Accuracy: 1.0000\n",
      "Epoch 55/100, Accuracy: 1.0000\n",
      "Epoch 56/100, Accuracy: 1.0000\n",
      "Epoch 57/100, Accuracy: 0.9667\n",
      "Epoch 58/100, Accuracy: 0.9667\n",
      "Epoch 59/100, Accuracy: 1.0000\n",
      "Epoch 60/100, Accuracy: 1.0000\n",
      "Epoch 61/100, Accuracy: 0.9667\n",
      "Epoch 62/100, Accuracy: 0.9667\n",
      "Epoch 63/100, Accuracy: 1.0000\n",
      "Epoch 64/100, Accuracy: 1.0000\n",
      "Epoch 65/100, Accuracy: 1.0000\n",
      "Epoch 66/100, Accuracy: 1.0000\n",
      "Epoch 67/100, Accuracy: 1.0000\n",
      "Epoch 68/100, Accuracy: 1.0000\n",
      "Epoch 69/100, Accuracy: 1.0000\n",
      "Epoch 70/100, Accuracy: 1.0000\n",
      "Epoch 71/100, Accuracy: 1.0000\n",
      "Epoch 72/100, Accuracy: 1.0000\n",
      "Epoch 73/100, Accuracy: 1.0000\n",
      "Epoch 74/100, Accuracy: 1.0000\n",
      "Epoch 75/100, Accuracy: 1.0000\n",
      "Epoch 76/100, Accuracy: 1.0000\n",
      "Epoch 77/100, Accuracy: 1.0000\n",
      "Epoch 78/100, Accuracy: 1.0000\n",
      "Epoch 79/100, Accuracy: 1.0000\n",
      "Epoch 80/100, Accuracy: 1.0000\n",
      "Epoch 81/100, Accuracy: 1.0000\n",
      "Epoch 82/100, Accuracy: 1.0000\n",
      "Epoch 83/100, Accuracy: 1.0000\n",
      "Epoch 84/100, Accuracy: 1.0000\n",
      "Epoch 85/100, Accuracy: 1.0000\n",
      "Epoch 86/100, Accuracy: 1.0000\n",
      "Epoch 87/100, Accuracy: 1.0000\n",
      "Epoch 88/100, Accuracy: 1.0000\n",
      "Epoch 89/100, Accuracy: 1.0000\n",
      "Epoch 90/100, Accuracy: 1.0000\n",
      "Epoch 91/100, Accuracy: 1.0000\n",
      "Epoch 92/100, Accuracy: 1.0000\n",
      "Epoch 93/100, Accuracy: 1.0000\n",
      "Epoch 94/100, Accuracy: 1.0000\n",
      "Epoch 95/100, Accuracy: 1.0000\n",
      "Epoch 96/100, Accuracy: 1.0000\n",
      "Epoch 97/100, Accuracy: 1.0000\n",
      "Epoch 98/100, Accuracy: 1.0000\n",
      "Epoch 99/100, Accuracy: 1.0000\n",
      "Epoch 100/100, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Muat dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Normalisasi fitur\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Bagi data menjadi train dan test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Persiapkan data untuk DataLoader\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(query, key, value, attn_mask=mask)[0]\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, embed_size, num_heads, forward_expansion, dropout):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size=embed_size,\n",
    "            heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            forward_expansion=forward_expansion\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_block(x, x, x, None)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "# Model\n",
    "model = TabularTransformer(\n",
    "    num_features=X.shape[1],\n",
    "    num_classes=len(set(y)),\n",
    "    embed_size=32,\n",
    "    num_heads=4,\n",
    "    forward_expansion=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Accuracy: {correct / total:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Accuracy: 0.3000\n",
      "Epoch 2/100, Accuracy: 0.9667\n",
      "Epoch 3/100, Accuracy: 0.9333\n",
      "Epoch 4/100, Accuracy: 0.9333\n",
      "Epoch 5/100, Accuracy: 0.9333\n",
      "Epoch 6/100, Accuracy: 0.9333\n",
      "Epoch 7/100, Accuracy: 0.9667\n",
      "Epoch 8/100, Accuracy: 1.0000\n",
      "Epoch 9/100, Accuracy: 1.0000\n",
      "Epoch 10/100, Accuracy: 1.0000\n",
      "Epoch 11/100, Accuracy: 1.0000\n",
      "Epoch 12/100, Accuracy: 1.0000\n",
      "Epoch 13/100, Accuracy: 0.9667\n",
      "Epoch 14/100, Accuracy: 0.9667\n",
      "Epoch 15/100, Accuracy: 1.0000\n",
      "Epoch 16/100, Accuracy: 1.0000\n",
      "Epoch 17/100, Accuracy: 1.0000\n",
      "Epoch 18/100, Accuracy: 1.0000\n",
      "Epoch 19/100, Accuracy: 1.0000\n",
      "Epoch 20/100, Accuracy: 1.0000\n",
      "Epoch 21/100, Accuracy: 1.0000\n",
      "Epoch 22/100, Accuracy: 1.0000\n",
      "Epoch 23/100, Accuracy: 0.9667\n",
      "Epoch 24/100, Accuracy: 1.0000\n",
      "Epoch 25/100, Accuracy: 1.0000\n",
      "Epoch 26/100, Accuracy: 1.0000\n",
      "Epoch 27/100, Accuracy: 1.0000\n",
      "Epoch 28/100, Accuracy: 1.0000\n",
      "Epoch 29/100, Accuracy: 1.0000\n",
      "Epoch 30/100, Accuracy: 1.0000\n",
      "Epoch 31/100, Accuracy: 1.0000\n",
      "Epoch 32/100, Accuracy: 1.0000\n",
      "Epoch 33/100, Accuracy: 0.9667\n",
      "Epoch 34/100, Accuracy: 1.0000\n",
      "Epoch 35/100, Accuracy: 1.0000\n",
      "Epoch 36/100, Accuracy: 0.9667\n",
      "Epoch 37/100, Accuracy: 1.0000\n",
      "Epoch 38/100, Accuracy: 1.0000\n",
      "Epoch 39/100, Accuracy: 1.0000\n",
      "Epoch 40/100, Accuracy: 1.0000\n",
      "Epoch 41/100, Accuracy: 1.0000\n",
      "Epoch 42/100, Accuracy: 1.0000\n",
      "Epoch 43/100, Accuracy: 1.0000\n",
      "Epoch 44/100, Accuracy: 0.9667\n",
      "Epoch 45/100, Accuracy: 1.0000\n",
      "Epoch 46/100, Accuracy: 0.9333\n",
      "Epoch 47/100, Accuracy: 1.0000\n",
      "Epoch 48/100, Accuracy: 0.9667\n",
      "Epoch 49/100, Accuracy: 1.0000\n",
      "Epoch 50/100, Accuracy: 1.0000\n",
      "Epoch 51/100, Accuracy: 1.0000\n",
      "Epoch 52/100, Accuracy: 1.0000\n",
      "Epoch 53/100, Accuracy: 1.0000\n",
      "Epoch 54/100, Accuracy: 1.0000\n",
      "Epoch 55/100, Accuracy: 0.9667\n",
      "Epoch 56/100, Accuracy: 1.0000\n",
      "Epoch 57/100, Accuracy: 1.0000\n",
      "Epoch 58/100, Accuracy: 1.0000\n",
      "Epoch 59/100, Accuracy: 1.0000\n",
      "Epoch 60/100, Accuracy: 1.0000\n",
      "Epoch 61/100, Accuracy: 1.0000\n",
      "Epoch 62/100, Accuracy: 1.0000\n",
      "Epoch 63/100, Accuracy: 1.0000\n",
      "Epoch 64/100, Accuracy: 1.0000\n",
      "Epoch 65/100, Accuracy: 1.0000\n",
      "Epoch 66/100, Accuracy: 1.0000\n",
      "Epoch 67/100, Accuracy: 1.0000\n",
      "Epoch 68/100, Accuracy: 1.0000\n",
      "Epoch 69/100, Accuracy: 1.0000\n",
      "Epoch 70/100, Accuracy: 1.0000\n",
      "Epoch 71/100, Accuracy: 1.0000\n",
      "Epoch 72/100, Accuracy: 1.0000\n",
      "Epoch 73/100, Accuracy: 1.0000\n",
      "Epoch 74/100, Accuracy: 1.0000\n",
      "Epoch 75/100, Accuracy: 1.0000\n",
      "Epoch 76/100, Accuracy: 1.0000\n",
      "Epoch 77/100, Accuracy: 1.0000\n",
      "Epoch 78/100, Accuracy: 1.0000\n",
      "Epoch 79/100, Accuracy: 1.0000\n",
      "Epoch 80/100, Accuracy: 1.0000\n",
      "Epoch 81/100, Accuracy: 1.0000\n",
      "Epoch 82/100, Accuracy: 1.0000\n",
      "Epoch 83/100, Accuracy: 1.0000\n",
      "Epoch 84/100, Accuracy: 1.0000\n",
      "Epoch 85/100, Accuracy: 1.0000\n",
      "Epoch 86/100, Accuracy: 1.0000\n",
      "Epoch 87/100, Accuracy: 1.0000\n",
      "Epoch 88/100, Accuracy: 1.0000\n",
      "Epoch 89/100, Accuracy: 1.0000\n",
      "Epoch 90/100, Accuracy: 1.0000\n",
      "Epoch 91/100, Accuracy: 1.0000\n",
      "Epoch 92/100, Accuracy: 1.0000\n",
      "Epoch 93/100, Accuracy: 1.0000\n",
      "Epoch 94/100, Accuracy: 0.9667\n",
      "Epoch 95/100, Accuracy: 0.9667\n",
      "Epoch 96/100, Accuracy: 1.0000\n",
      "Epoch 97/100, Accuracy: 1.0000\n",
      "Epoch 98/100, Accuracy: 1.0000\n",
      "Epoch 99/100, Accuracy: 1.0000\n",
      "Epoch 100/100, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Muat dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Normalisasi fitur\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Bagi data menjadi train dan test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Persiapkan data untuk DataLoader\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, embed_size, num_heads, num_layers, forward_expansion, dropout):\n",
    "        super(SAINT, self).__init__()\n",
    "        self.embedding = nn.Linear(num_features, embed_size)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=forward_expansion,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = x.squeeze(1)  # Remove sequence dimension\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "model = SAINT(\n",
    "    num_features=X.shape[1],\n",
    "    num_classes=len(set(y)),\n",
    "    embed_size=32,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    forward_expansion=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Accuracy: {correct / total:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SAINTModel' from 'pytorch_tabular.models' (c:\\Users\\fadhil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabular\\models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_tabular\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAINTModel\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SAINTModel' from 'pytorch_tabular.models' (c:\\Users\\fadhil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_tabular\\models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pytorch_tabular.models import SAINTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModelConfig.__init__() got an unexpected keyword argument 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     19\u001b[0m data_config \u001b[38;5;241m=\u001b[39m DataConfig(\n\u001b[0;32m     20\u001b[0m     target\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Nama kolom target\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     continuous_cols\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist(),  \u001b[38;5;66;03m# Nama kolom fitur numerik\u001b[39;00m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Konfigurasikan model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mModelConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tugas: \"classification\"\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Nama model: gunakan string \"saint\" untuk SAINT\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Konfigurasikan optimisasi\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer_config \u001b[38;5;241m=\u001b[39m OptimizerConfig()\n",
      "\u001b[1;31mTypeError\u001b[0m: ModelConfig.__init__() got an unexpected keyword argument 'model_name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig, ModelConfig, OptimizerConfig, TrainerConfig\n",
    "\n",
    "# Muat dataset Iris\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='target')\n",
    "\n",
    "# Gabungkan fitur dan target ke dalam satu DataFrame\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Bagi data menjadi train dan test\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Konfigurasikan data\n",
    "data_config = DataConfig(\n",
    "    target=[\"target\"],  # Nama kolom target\n",
    "    continuous_cols=X.columns.tolist(),  # Nama kolom fitur numerik\n",
    ")\n",
    "\n",
    "# Konfigurasikan model\n",
    "model_config = ModelConfig(\n",
    "    task=\"classification\",  # Tugas: \"classification\"\n",
    "    model_name=\"saint\",  # Nama model: gunakan string \"saint\" untuk SAINT\n",
    ")\n",
    "\n",
    "# Konfigurasikan optimisasi\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "# Konfigurasikan pelatihan\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    max_epochs=100,\n",
    ")\n",
    "\n",
    "# Inisialisasi model dengan konfigurasi di atas\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "# Pelatihan model\n",
    "tabular_model.fit(train=train_data)\n",
    "\n",
    "# Evaluasi model\n",
    "result = tabular_model.evaluate(test=test_data)\n",
    "print(result)\n",
    "\n",
    "# Prediksi menggunakan model terlatih\n",
    "predictions = tabular_model.predict(test_data)\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0929\n",
      "Epoch [20/100], Loss: 1.0782\n",
      "Epoch [30/100], Loss: 0.9923\n",
      "Epoch [40/100], Loss: 0.6962\n",
      "Epoch [50/100], Loss: 0.4592\n",
      "Epoch [60/100], Loss: 0.2658\n",
      "Epoch [70/100], Loss: 0.1123\n",
      "Epoch [80/100], Loss: 0.0710\n",
      "Epoch [90/100], Loss: 0.0583\n",
      "Epoch [100/100], Loss: 0.0552\n",
      "Train Accuracy: 0.9905\n",
      "Test Accuracy: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class ObliviousDecisionTreeLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_trees=10, tree_depth=6):\n",
    "        super(ObliviousDecisionTreeLayer, self).__init__()\n",
    "        self.num_trees = num_trees\n",
    "        self.tree_depth = tree_depth\n",
    "        self.weight = nn.Parameter(torch.randn(num_trees, tree_depth, input_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(num_trees, tree_depth))  # Bias berbentuk [num_trees, tree_depth]\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(self.num_trees):\n",
    "            out = x\n",
    "            for j in range(self.tree_depth):\n",
    "                # Perluas 'bias' agar sesuai dengan dimensi batch\n",
    "                bias = self.bias[i, j].unsqueeze(0).expand(batch_size, -1)  # Bias diperluas agar sesuai dengan batch_size\n",
    "                # Operasikan linear layer dengan dimensi yang cocok\n",
    "                decision = torch.sigmoid(F.linear(out, self.weight[i, j].unsqueeze(0), bias))\n",
    "                # Sesuaikan dimensi 'out' untuk melanjutkan operasi\n",
    "                out = decision * out\n",
    "            outputs.append(out)\n",
    "\n",
    "        # Rata-rata output dari semua pohon\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "# Definisi NODE Model\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_trees=10, tree_depth=6, hidden_dim=64):\n",
    "        super(NODE, self).__init__()\n",
    "        self.tree_layer = ObliviousDecisionTreeLayer(input_dim, num_trees=num_trees, tree_depth=tree_depth)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Sesuaikan input dimension sesuai dengan output dari tree_layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tree_layer(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Persiapan data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Konversi ke tensor\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Inisialisasi model NODE\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(set(y))\n",
    "model = NODE(input_dim, output_dim, num_trees=10, tree_depth=6, hidden_dim=64)\n",
    "\n",
    "# Definisi optimizer dan loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_t)\n",
    "    loss = criterion(outputs, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluasi model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = torch.argmax(model(X_train_t), dim=1)\n",
    "    y_pred_test = torch.argmax(model(X_test_t), dim=1)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_pred_train.numpy())\n",
    "    test_acc = accuracy_score(y_test, y_pred_test.numpy())\n",
    "    f1 = f1_score(y_test, y_pred_test.numpy(), average='weighted')\n",
    "\n",
    "    print(f'Train Accuracy: {train_acc:.4f}')\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NODEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Oblivious Decision Ensembles (NODE) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_trees, depth):\n",
    "        \"\"\"\n",
    "        Initialize the NODE model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            num_trees (int): Number of decision trees in the ensemble.\n",
    "            depth (int): Depth of each decision tree.\n",
    "        \"\"\"\n",
    "        super(NODEModel, self).__init__()\n",
    "        self.num_trees = num_trees\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Create a list of decision trees\n",
    "        self.trees = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(input_dim, 2 ** depth),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 ** depth, 1)\n",
    "        ) for _ in range(num_trees)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
